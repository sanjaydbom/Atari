# Rainbow DQN

### Abstract
This project aimed to recreate the [Rainbow DQN paper]() by Google Deepmind from 2017. This project incorporates 7 new techniques applied to the standard dqn to significantly improve the performace. I then trained the new model on Atari Breakout to test the performance and got the best score of ______, significantly beating the human level performance of 30. This shows that my model is able to learn complicated states and interactions, paving way for future models to help in real world scenarios that involve interactions among many moving parts.

### Background
The original idea for this type of model learning, called Q-Learning is introduce in [Reinfocement Learning: An Introduction]() by Sutton and Barto. In this book, they explain that a Q-Learning model is simply interacting with an environment numerous times, and storing the "value" of each action in a given state by adding the reward recieved by taking an action to the value of the next state. As the agent explores more states and actions, it gets a better estimate on the true values of taking each action in a given state. Then, once done training, the model can simply choose the action that returns the highest value in a given state to get the optimal solution. However this has a major problem of needing to store each state-action pair in a table, which grows exponentially as the environment gets more complicated. This makes it unsuitable for real world tasks where the environment is has thousands of degrees of freedom

To fix this problem, researchers at Google Deepmind released the [DQN]() or Deep Q-Learning model. This model uses a Neural Network instead of a table to store the state action values. This decreases the memory requirements to a single constant, namely the size of the neural network, which we can set. One problem with this setup is that if we update the weights to change the value of a state-action pair, this affects all other state action pairs since the weights are shared. However, over time the weights converge to values that properly reflect all state action values. Another problem is that we need significantly more interactions with the environment to properly capture all state action values due to the nature of the shifting weights. To fix this, researchs introduced an experience replay, which stores the last n experiences, and randonly chooses m experiences, where m is the mini batch size, and trains the model on them for that step. This is analogous to memory in humans, and allows the model to reuse experiences instead of discarding them immediately. Finally, to stabilize training, we use a target model to provide an estimate for the DQN. This target model is updated to the current model every couple thousand steps to keep the targets close to the actualy values without constantly drifting

The next improvement was to introduce [Prioritized Replay](). We simply use the TD error, or how off our estimate was from the actual value of the state action pair, as a proxy of how "suprising" each experience was. This allows us to select the interactions that our agent doesn't capture well, and train on those interactions specifcally, speeding up training. This is also similar to human memory, as we don't remember and think about bland memories but rather ones that evoke a lot of emotion, such as a moment of bliss or embaresment. 

A novel improvement was called [Dueling DQN's](). In this architecture, the values of the each state were seperated from the advantage of each action, or the relative ranking of all the actions in that state. This allows the model to differentiate between states all actions essentially lead to the same outcome, such as a Atari breakout when the ball is going up, and states where one actions is vastly better than others, such as reflecting the ball. 

Another novel improvement concerned how the model explores different states to get an accurate representation of the state action pairs called [Noisy Nets](). Previously, all models used the exploration technique called epsilon exploration, where the model randomly chooses and action with probability epsilon, and chooses the best action with probability 1 - epsilon. If we start with a high epsilon, and decay it as the agent learns, we get a nice tradeoff between exploration- exploring new states, and exploitation-Choosing the best state to understand them better. Now, instead of choosing the epsilon and how to decay it, we let the model control the epsilon, thereby allowing the model to explore if it thinks it needs to know more about the environment, and exploit if it thinks it has reached the most optimal solution.

The penultimate improvement, also described in Reinforcement Learning: An Introduction is n-step learning. Instead of just looking forward a single action to determine the value of the current state, we look forward n steps in the future to get a better estimate of state-action pairs. This is similar to how a good chess player looks forward many steps when playing a move compared to the novice who looks forward only one move.

The final improvement, are [Distributional DQN's](). In this model, instead of just outputting a single value of the expected return for a single state action pair, we return a distributions of values. This obviously allows us to capture more complicated systems of interactions, such as when 2 state-actions have the same expected return, but one returns 1 point 50% of the time, and -1 50% of the time, compared to one that returns 0 100% of the time. In different times, different actions are preferred. Even in deterministic systems, allows the model to get a better estimate of the environment, allowing it to perform better.

